{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Desarrollo del ChatBot (LLM)**"
      ],
      "metadata": {
        "id": "OIW5JLQccuSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso: 1 Descarga de paquetes**<p>\n",
        "Realizamos la instalación de las librerías necesarias usando pip. Se incluyen dos paquetes principalmente: <p>\n",
        "1. **python-telegram-bot**: Permite la interacción con la API de Telegram para implemetnar el bot conversacional. Se fija en la versión 13.15 para asegurar compatibilidad.\n",
        "2. **openai**: Proporciona la interfaz para consultar los modelos de OpenAI (GPT-3.5 y GPT-4). Se instala la versión 0.28"
      ],
      "metadata": {
        "id": "swdmzfYfRWrQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHxinlKp_qDo",
        "outputId": "5db5359e-7212-4011-9e5c-a8e50bb7c918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-telegram-bot==13.15 in /usr/local/lib/python3.11/dist-packages (13.15)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (2025.4.26)\n",
            "Requirement already satisfied: tornado==6.1 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (6.1)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (3.6.3)\n",
            "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (2025.2)\n",
            "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.15) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.15) (1.17.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.15) (5.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-telegram-bot==13.15 openai==0.28\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 2: Carga de claves API y configuración**<p>\n",
        "Las celdas siguientes configuran las claves de API y otros parámetros necesarios. En primer lugar, en un contexto de Colab se asignan las claves directamente a variables de entorno para esta sesión. Posteriormente, se lee cada clave de API desde el entorno y se verifica su existencia."
      ],
      "metadata": {
        "id": "QC9CeLaHSc5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQWOXHmPFuER"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"TELEGRAM_BOT_TOKEN\"] = \"7404286465:AAF_jNQv04N_2qOPQo7ZMnCwLVmK3cMlDlc\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGTeTtP9_4v8"
      },
      "outputs": [],
      "source": [
        "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "telegram_token  = os.environ.get(\"TELEGRAM_BOT_TOKEN\")\n",
        "assert openai_api_key is not None, \"Falta configurar OPENAI_API_KEY\"\n",
        "assert telegram_token is not None, \"Falta configurar TELEGRAM_BOT_TOKEN\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 3: Definición de funciones de consulta a modelos OpenAI**<p>\n",
        "Tras abonar 5 euros para acceder a las claves de la API de OpenAI, hemos podido evaluar los principales modelos disponibles actualmente en la plataforma. Los más destacados son los siguientes:\n",
        "\n",
        "1. **GPT-3.5 Turbo** (gpt-3.5-turbo)\n",
        "Modelo rápido, económico y con un rendimiento notable. Es especialmente adecuado para el desarrollo de chatbots, asistentes virtuales y tareas de procesamiento ligero.\n",
        "2. **GPT-4 Turbo** (gpt-4-turbo)\n",
        "Un modelo más avanzado, con gran capacidad para manejar contextos extensos y resolver tareas complejas con alta precisión.\n",
        "3. **GPT-4o** (gpt-4o)\n",
        "La versión más reciente y optimizada. Ofrece un rendimiento comparable al de GPT-4 Turbo, pero con mayor velocidad y menor coste, lo que lo convierte en una opción muy eficiente.<p>\n",
        "\n",
        "Cómo solo empleamos OpenAI como proveedor de modelos LLM, necesitamos desarrollar una única función capaz de elaborar una respuesta acorde al prompt del usuario (nosotros)."
      ],
      "metadata": {
        "id": "694K5zuBTDgY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vUobpGo_70z"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def get_response_from_openai_model(user_message: str, model: str) -> str:\n",
        "    try:\n",
        "\n",
        "        # Definir el prompt con un rol de sistema opcional para guiar la respuesta\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Eres un asistente útil y conciso.\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "        response = openai.ChatCompletion.create(model=model, messages=messages)\n",
        "\n",
        "        # Extraer el texto de la respuesta del asistente\n",
        "        answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        return answer.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error al consultar OpenAI: {e}\")\n",
        "        return \"Lo siento, no pude obtener respuesta de OpenAI.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 4: Combinación de respuestas**<p>\n",
        "A continuación, definimos una función que recibe como entrada el mensaje original del usuario y las dos respuestas generadas previamente. Su objetivo es producir una única respuesta consensuada a partir de esa información.\n",
        "\n",
        "Internamente, la función construye un *prompt* que incorpora tanto la pregunta como ambas respuestas, solicitando al modelo GPT-4o que las integre en una respuesta única, coherente y unificada.\n",
        "\n",
        "Es importante señalar que el *prompt* utilizado por la función *generate_consensus_response* sigue una plantilla previamente definida por nosotros, diseñada para garantizar consistencia en los resultados."
      ],
      "metadata": {
        "id": "fLTJLlNOYHWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qmdC1Qr_-Ej"
      },
      "outputs": [],
      "source": [
        "def generate_consensus_response(user_message: str, answer1: str, answer2: str, model: str = \"gpt-4o\") -> str:\n",
        "    try:\n",
        "        consensus_prompt = (\n",
        "            \"El usuario preguntó lo siguiente:\\n\"\n",
        "            f\"{user_message}\\n\\n\"\n",
        "            \"Dos asistentes propusieron las siguientes respuestas.\\n\"\n",
        "            f\"Respuesta 1: {answer1}\\n\\n\"\n",
        "            f\"Respuesta 2: {answer2}\\n\\n\"\n",
        "            \"Como asistente experto, combina la información de las dos respuestas en una sola respuesta coherente, completa y consensuada para el usuario. \"\n",
        "            \"No menciones que hubo más de una respuesta, solo provee la mejor respuesta posible unificando ambas.\"\n",
        "        )\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Eres un modelo que genera respuestas consensuadas a partir de varias respuestas de IA.\"},\n",
        "            {\"role\": \"user\", \"content\": consensus_prompt}\n",
        "        ]\n",
        "        response = openai.ChatCompletion.create(model=model, messages=messages)\n",
        "        final_answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        return final_answer.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al generar respuesta consensuada: {e}\")\n",
        "        return \"Lo siento, ocurrió un error al generar la respuesta final.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 5: Registro de conversaciones**<p>\n",
        "En determinadas situaciones, resulta especialmente útil almacenar el historial de interacciones del chatbot en un archivo *CSV*. Este archivo puede incluir elementos clave como el *timestamp*, el mensaje original del usuario, las dos respuestas generadas por distintos modelos y la respuesta consensuada final.\n",
        "\n",
        "Este registro no solo permite llevar un control detallado del funcionamiento del sistema, sino que adquiere un valor añadido en contextos donde el objetivo es diseñar un modelo de lenguaje que actúe como copiloto o guía, orientando al usuario en lugar de limitarse a proporcionar soluciones inmediatas. Mediante el análisis posterior del historial, es posible evaluar si el comportamiento del *chatbot* se ajusta a este enfoque o si tiende a comportarse más como un buscador tradicional.\n",
        "\n",
        "Además, conservar estos datos resulta fundamental para procesos de mejora continua del modelo. El historial puede emplearse como fuente de datos para reentrenamiento o ajuste fino (*fine-tuning*), permitiendo que el *chatbot* aprenda de sus propias interacciones y mejore progresivamente su precisión, utilidad y alineación con los objetivos del sistema."
      ],
      "metadata": {
        "id": "oqva3YEUaFnb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atR5FjL6___O"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "log_file = \"Chat_History.csv\"\n",
        "if not os.path.exists(log_file):\n",
        "    with open(log_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"timestamp\", \"user_message\", \"openai_response\", \"final_response\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_interaction(user_message: str, answer1: str, answer2: str, final_answer: str):\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([timestamp, user_message, answer1, answer2, final_answer])"
      ],
      "metadata": {
        "id": "rJfvo5oWTUIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 6: Funcionamiento e inicialización de nuestro ChatBot**"
      ],
      "metadata": {
        "id": "UqexJs6ta-MP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux4yj4_KACG6"
      },
      "outputs": [],
      "source": [
        "from telegram.ext import Updater, MessageHandler, Filters\n",
        "\n",
        "def handle_message(update, context):\n",
        "\n",
        "    user_msg = update.message.text\n",
        "    print(f\"[Recibido] Usuario dijo: {user_msg}\")\n",
        "\n",
        "    # 1. Obtener respuestas de ambos modelos\n",
        "    response1 = get_response_from_openai_model(user_msg, model=\"gpt-3.5-turbo\")\n",
        "    response2 = get_response_from_openai_model(user_msg, model=\"gpt-4-turbo\")\n",
        "    print(f\"[Debug] Respuesta OpenAI (GPT 3.5 turbo): {response1}\")\n",
        "    print(f\"[Debug] Respuesta OpenAI (GPT 4 turbo): {response2}\")\n",
        "\n",
        "    # 2. Generar respuesta consensuada\n",
        "    final_response = generate_consensus_response(user_msg, response1, response2)\n",
        "    print(f\"[Debug] Respuesta final: {final_response}\")\n",
        "\n",
        "    # 3. Registrar en el log CSV\n",
        "    log_interaction(user_msg, response1, response2, final_response)\n",
        "\n",
        "    # 4. Enviar la respuesta final al usuario\n",
        "    update.message.reply_text(final_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOvGVTDjAEMe",
        "outputId": "0a4fcfa0-195d-4b9b-cf79-a66b19ac477f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando el bot de Telegram... Esperando mensajes.\n",
            "[Recibido] Usuario dijo: Hola Chat\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): Hola, ¿cómo puedo ayudarte hoy?\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): Hola! ¿En qué puedo ayudarte hoy?\n",
            "[Debug] Respuesta final: Hola, ¿en qué puedo ayudarte hoy?\n",
            "[Recibido] Usuario dijo: Estoy haciendo una prueba de funcionamiento para que mi profesor vea que estás bien hecho\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): ¡Genial! ¿Necesitas ayuda o tienes alguna pregunta específica sobre la prueba de funcionamiento que estás realizando?\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): ¡Perfecto! Si tienes alguna pregunta o necesitas que realice alguna tarea, estoy aquí para ayudarte. ¿Cómo puedo asistirte hoy?\n",
            "[Debug] Respuesta final: ¡Perfecto! ¿Necesitas ayuda o tienes alguna pregunta específica sobre la prueba de funcionamiento que estás realizando? Estoy aquí para asistirte en lo que necesites.\n",
            "[Recibido] Usuario dijo: Me gustaría que me dijeses cuánto es 5 + 7\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): La suma de 5 + 7 es igual a 12.\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): 5 + 7 es igual a 12.\n",
            "[Debug] Respuesta final: La suma de 5 + 7 es igual a 12.\n",
            "[Recibido] Usuario dijo: muy bien muy bien\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): Me alegra que estés contento. ¿En qué más puedo ayudarte hoy?\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): ¡Qué bueno! ¿En qué puedo ayudarte hoy?\n",
            "[Debug] Respuesta final: ¡Qué bueno que estés contento! ¿En qué más puedo ayudarte hoy?\n",
            "[Recibido] Usuario dijo: suficiente jeje\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): ¡Me alegra haberte sido de ayuda! Si tienes alguna otra pregunta, ¡estaré aquí para ayudarte! 😊\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): ¡Entendido! ¿En qué más puedo ayudarte hoy?\n",
            "[Debug] Respuesta final: ¡Me alegra haber sido de ayuda! Si necesitas algo más, estoy aquí para asistirte. 😊 ¿En qué más puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "# Configurar el bot de Telegram y los handlers\n",
        "updater = Updater(telegram_token, use_context=True)\n",
        "dispatcher = updater.dispatcher\n",
        "\n",
        "# Registrar el manejador para mensajes de texto (no comandos)\n",
        "dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_message))\n",
        "\n",
        "# Iniciar el polling del bot\n",
        "print(\"Iniciando el bot de Telegram... Esperando mensajes.\")\n",
        "updater.start_polling()\n",
        "updater.idle()  # Mantener el bot corriendo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aprendizajes y líneas futuras**"
      ],
      "metadata": {
        "id": "VvrQjAonc0Rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Uso de API de Claude**\n",
        "\n",
        "Aunque inicialmente intentamos utilizar la API de pago de Claude (Anthropic) para enriquecer la diversidad de respuestas generadas por modelos de diferentes proveedores, la implementación resultó inviable. A pesar de varios intentos, la integración fallaba sistemáticamente debido a un error persistente que no pudimos resolver dentro del tiempo y recursos disponibles.\n",
        "\n",
        "Como consecuencia, optamos por adaptar la arquitectura del sistema para combinar respuestas de modelos distintos dentro del ecosistema de OpenAI, concretamente GPT-3.5 y GPT-4. Esta alternativa permitió mantener la funcionalidad principal del sistema —la generación de una respuesta consensuada— sin comprometer la estabilidad del servicio.\n",
        "\n",
        "A futuro, sería interesante explorar la combinación de modelos de distintas compañías, como Mistral o DeepSeek, lo que no solo aportaría mayor robustez y variedad en los estilos de respuesta, sino que también permitiría evaluar el comportamiento de múltiples LLMs en contextos comparables."
      ],
      "metadata": {
        "id": "0gDBalmDc4ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Seguridad de las credenciales API**\n",
        "\n",
        "Un problema crítico identificado en el desarrollo actual del proyecto es la exposición directa de la clave de la API de OpenAI dentro del notebook. En su estado actual, esta clave se encuentra codificada de forma explícita en una de las celdas del *notebook*, lo que implica que cualquier persona con acceso al archivo puede visualizarla y utilizarla sin restricciones.\n",
        "\n",
        "Este problema tiene implicaciones tanto funcionales como económicas:\n",
        "\n",
        "1. **Riesgo financiero**: La clave de API de OpenAI está siendo financiada actualmente de forma personal por nosotros (Yago). La exposición de dicha clave permitiría que terceros generen peticiones a la API con cargo a esa cuenta, lo cual podría generar costes inesperados y no autorizados.\n",
        "2. **Falta de control de acceso**: Cualquier persona que reciba el notebook (por ejemplo, por correo, Drive o GitHub) puede copiar o ejecutar la clave sin ningún mecanismo de autenticación.\n",
        "3. **Riesgos de seguridad y reputación**: Si la clave se utiliza para realizar acciones automatizadas fuera del control del propietario, también podría comprometer el uso legítimo del servicio o incluso suponer una violación de los términos de uso de OpenAI."
      ],
      "metadata": {
        "id": "bRSw0D4Kd1ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Especificidad del ChatBot**\n",
        "\n",
        "Los *chatbots* han dejado de ser una idea futurista para convertirse en una realidad ampliamente implementada. En la actualidad, existen numerosos asistentes conversacionales de propósito general, cada uno con diferentes niveles de utilidad y especialización. Esta abundancia de opciones plantea el reto de cómo diferenciar una nueva propuesta dentro de un ecosistema ya saturado.\n",
        "\n",
        "Una estrategia eficaz para aportar valor añadido sería realizar un proceso de *fine-tuning* del modelo base utilizando datos específicos y relevantes para un contexto concreto. Por ejemplo, podría entrenarse el modelo con el contenido académico de las distintas asignaturas de una carrera universitaria. De esta forma, se desarrollaría un asistente virtual educativo, capaz de responder dudas frecuentes de los estudiantes de manera rápida, precisa y personalizada.\n",
        "\n",
        "Este enfoque permitiría al *chatbot* actuar como un complemento al profesor, especialmente útil para resolver consultas sencillas o recurrentes, que a menudo no justifican un contacto directo. Así, se aliviaría la carga de los docentes y se mejoraría la experiencia del alumno, promoviendo un aprendizaje más autónomo y accesible."
      ],
      "metadata": {
        "id": "qTbJjUfYfKBG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}