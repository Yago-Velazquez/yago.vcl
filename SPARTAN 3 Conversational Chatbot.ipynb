{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Desarrollo del ChatBot (LLM)**"
      ],
      "metadata": {
        "id": "OIW5JLQccuSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso: 1 Descarga de paquetes**<p>\n",
        "Realizamos la instalaci√≥n de las librer√≠as necesarias usando pip. Se incluyen dos paquetes principalmente: <p>\n",
        "1. **python-telegram-bot**: Permite la interacci√≥n con la API de Telegram para implemetnar el bot conversacional. Se fija en la versi√≥n 13.15 para asegurar compatibilidad.\n",
        "2. **openai**: Proporciona la interfaz para consultar los modelos de OpenAI (GPT-3.5 y GPT-4). Se instala la versi√≥n 0.28"
      ],
      "metadata": {
        "id": "swdmzfYfRWrQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHxinlKp_qDo",
        "outputId": "5db5359e-7212-4011-9e5c-a8e50bb7c918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-telegram-bot==13.15 in /usr/local/lib/python3.11/dist-packages (13.15)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (2025.4.26)\n",
            "Requirement already satisfied: tornado==6.1 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (6.1)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (3.6.3)\n",
            "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (2025.2)\n",
            "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot==13.15) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.15) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.15) (1.17.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.11/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.15) (5.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-telegram-bot==13.15 openai==0.28\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 2: Carga de claves API y configuraci√≥n**<p>\n",
        "Las celdas siguientes configuran las claves de API y otros par√°metros necesarios. En primer lugar, en un contexto de Colab se asignan las claves directamente a variables de entorno para esta sesi√≥n. Posteriormente, se lee cada clave de API desde el entorno y se verifica su existencia."
      ],
      "metadata": {
        "id": "QC9CeLaHSc5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQWOXHmPFuER"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"TELEGRAM_BOT_TOKEN\"] = \"7404286465:AAF_jNQv04N_2qOPQo7ZMnCwLVmK3cMlDlc\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGTeTtP9_4v8"
      },
      "outputs": [],
      "source": [
        "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "telegram_token  = os.environ.get(\"TELEGRAM_BOT_TOKEN\")\n",
        "assert openai_api_key is not None, \"Falta configurar OPENAI_API_KEY\"\n",
        "assert telegram_token is not None, \"Falta configurar TELEGRAM_BOT_TOKEN\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 3: Definici√≥n de funciones de consulta a modelos OpenAI**<p>\n",
        "Tras abonar 5 euros para acceder a las claves de la API de OpenAI, hemos podido evaluar los principales modelos disponibles actualmente en la plataforma. Los m√°s destacados son los siguientes:\n",
        "\n",
        "1. **GPT-3.5 Turbo** (gpt-3.5-turbo)\n",
        "Modelo r√°pido, econ√≥mico y con un rendimiento notable. Es especialmente adecuado para el desarrollo de chatbots, asistentes virtuales y tareas de procesamiento ligero.\n",
        "2. **GPT-4 Turbo** (gpt-4-turbo)\n",
        "Un modelo m√°s avanzado, con gran capacidad para manejar contextos extensos y resolver tareas complejas con alta precisi√≥n.\n",
        "3. **GPT-4o** (gpt-4o)\n",
        "La versi√≥n m√°s reciente y optimizada. Ofrece un rendimiento comparable al de GPT-4 Turbo, pero con mayor velocidad y menor coste, lo que lo convierte en una opci√≥n muy eficiente.<p>\n",
        "\n",
        "C√≥mo solo empleamos OpenAI como proveedor de modelos LLM, necesitamos desarrollar una √∫nica funci√≥n capaz de elaborar una respuesta acorde al prompt del usuario (nosotros)."
      ],
      "metadata": {
        "id": "694K5zuBTDgY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vUobpGo_70z"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "def get_response_from_openai_model(user_message: str, model: str) -> str:\n",
        "    try:\n",
        "\n",
        "        # Definir el prompt con un rol de sistema opcional para guiar la respuesta\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Eres un asistente √∫til y conciso.\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "        response = openai.ChatCompletion.create(model=model, messages=messages)\n",
        "\n",
        "        # Extraer el texto de la respuesta del asistente\n",
        "        answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        return answer.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error al consultar OpenAI: {e}\")\n",
        "        return \"Lo siento, no pude obtener respuesta de OpenAI.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 4: Combinaci√≥n de respuestas**<p>\n",
        "A continuaci√≥n, definimos una funci√≥n que recibe como entrada el mensaje original del usuario y las dos respuestas generadas previamente. Su objetivo es producir una √∫nica respuesta consensuada a partir de esa informaci√≥n.\n",
        "\n",
        "Internamente, la funci√≥n construye un *prompt* que incorpora tanto la pregunta como ambas respuestas, solicitando al modelo GPT-4o que las integre en una respuesta √∫nica, coherente y unificada.\n",
        "\n",
        "Es importante se√±alar que el *prompt* utilizado por la funci√≥n *generate_consensus_response* sigue una plantilla previamente definida por nosotros, dise√±ada para garantizar consistencia en los resultados."
      ],
      "metadata": {
        "id": "fLTJLlNOYHWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qmdC1Qr_-Ej"
      },
      "outputs": [],
      "source": [
        "def generate_consensus_response(user_message: str, answer1: str, answer2: str, model: str = \"gpt-4o\") -> str:\n",
        "    try:\n",
        "        consensus_prompt = (\n",
        "            \"El usuario pregunt√≥ lo siguiente:\\n\"\n",
        "            f\"{user_message}\\n\\n\"\n",
        "            \"Dos asistentes propusieron las siguientes respuestas.\\n\"\n",
        "            f\"Respuesta 1: {answer1}\\n\\n\"\n",
        "            f\"Respuesta 2: {answer2}\\n\\n\"\n",
        "            \"Como asistente experto, combina la informaci√≥n de las dos respuestas en una sola respuesta coherente, completa y consensuada para el usuario. \"\n",
        "            \"No menciones que hubo m√°s de una respuesta, solo provee la mejor respuesta posible unificando ambas.\"\n",
        "        )\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Eres un modelo que genera respuestas consensuadas a partir de varias respuestas de IA.\"},\n",
        "            {\"role\": \"user\", \"content\": consensus_prompt}\n",
        "        ]\n",
        "        response = openai.ChatCompletion.create(model=model, messages=messages)\n",
        "        final_answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        return final_answer.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al generar respuesta consensuada: {e}\")\n",
        "        return \"Lo siento, ocurri√≥ un error al generar la respuesta final.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 5: Registro de conversaciones**<p>\n",
        "En determinadas situaciones, resulta especialmente √∫til almacenar el historial de interacciones del chatbot en un archivo *CSV*. Este archivo puede incluir elementos clave como el *timestamp*, el mensaje original del usuario, las dos respuestas generadas por distintos modelos y la respuesta consensuada final.\n",
        "\n",
        "Este registro no solo permite llevar un control detallado del funcionamiento del sistema, sino que adquiere un valor a√±adido en contextos donde el objetivo es dise√±ar un modelo de lenguaje que act√∫e como copiloto o gu√≠a, orientando al usuario en lugar de limitarse a proporcionar soluciones inmediatas. Mediante el an√°lisis posterior del historial, es posible evaluar si el comportamiento del *chatbot* se ajusta a este enfoque o si tiende a comportarse m√°s como un buscador tradicional.\n",
        "\n",
        "Adem√°s, conservar estos datos resulta fundamental para procesos de mejora continua del modelo. El historial puede emplearse como fuente de datos para reentrenamiento o ajuste fino (*fine-tuning*), permitiendo que el *chatbot* aprenda de sus propias interacciones y mejore progresivamente su precisi√≥n, utilidad y alineaci√≥n con los objetivos del sistema."
      ],
      "metadata": {
        "id": "oqva3YEUaFnb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atR5FjL6___O"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "log_file = \"Chat_History.csv\"\n",
        "if not os.path.exists(log_file):\n",
        "    with open(log_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"timestamp\", \"user_message\", \"openai_response\", \"final_response\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_interaction(user_message: str, answer1: str, answer2: str, final_answer: str):\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([timestamp, user_message, answer1, answer2, final_answer])"
      ],
      "metadata": {
        "id": "rJfvo5oWTUIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 6: Funcionamiento e inicializaci√≥n de nuestro ChatBot**"
      ],
      "metadata": {
        "id": "UqexJs6ta-MP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux4yj4_KACG6"
      },
      "outputs": [],
      "source": [
        "from telegram.ext import Updater, MessageHandler, Filters\n",
        "\n",
        "def handle_message(update, context):\n",
        "\n",
        "    user_msg = update.message.text\n",
        "    print(f\"[Recibido] Usuario dijo: {user_msg}\")\n",
        "\n",
        "    # 1. Obtener respuestas de ambos modelos\n",
        "    response1 = get_response_from_openai_model(user_msg, model=\"gpt-3.5-turbo\")\n",
        "    response2 = get_response_from_openai_model(user_msg, model=\"gpt-4-turbo\")\n",
        "    print(f\"[Debug] Respuesta OpenAI (GPT 3.5 turbo): {response1}\")\n",
        "    print(f\"[Debug] Respuesta OpenAI (GPT 4 turbo): {response2}\")\n",
        "\n",
        "    # 2. Generar respuesta consensuada\n",
        "    final_response = generate_consensus_response(user_msg, response1, response2)\n",
        "    print(f\"[Debug] Respuesta final: {final_response}\")\n",
        "\n",
        "    # 3. Registrar en el log CSV\n",
        "    log_interaction(user_msg, response1, response2, final_response)\n",
        "\n",
        "    # 4. Enviar la respuesta final al usuario\n",
        "    update.message.reply_text(final_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOvGVTDjAEMe",
        "outputId": "0a4fcfa0-195d-4b9b-cf79-a66b19ac477f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando el bot de Telegram... Esperando mensajes.\n",
            "[Recibido] Usuario dijo: Hola Chat\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): Hola, ¬øc√≥mo puedo ayudarte hoy?\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): Hola! ¬øEn qu√© puedo ayudarte hoy?\n",
            "[Debug] Respuesta final: Hola, ¬øen qu√© puedo ayudarte hoy?\n",
            "[Recibido] Usuario dijo: Estoy haciendo una prueba de funcionamiento para que mi profesor vea que est√°s bien hecho\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): ¬°Genial! ¬øNecesitas ayuda o tienes alguna pregunta espec√≠fica sobre la prueba de funcionamiento que est√°s realizando?\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): ¬°Perfecto! Si tienes alguna pregunta o necesitas que realice alguna tarea, estoy aqu√≠ para ayudarte. ¬øC√≥mo puedo asistirte hoy?\n",
            "[Debug] Respuesta final: ¬°Perfecto! ¬øNecesitas ayuda o tienes alguna pregunta espec√≠fica sobre la prueba de funcionamiento que est√°s realizando? Estoy aqu√≠ para asistirte en lo que necesites.\n",
            "[Recibido] Usuario dijo: Me gustar√≠a que me dijeses cu√°nto es 5 + 7\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): La suma de 5 + 7 es igual a 12.\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): 5 + 7 es igual a 12.\n",
            "[Debug] Respuesta final: La suma de 5 + 7 es igual a 12.\n",
            "[Recibido] Usuario dijo: muy bien muy bien\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): Me alegra que est√©s contento. ¬øEn qu√© m√°s puedo ayudarte hoy?\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): ¬°Qu√© bueno! ¬øEn qu√© puedo ayudarte hoy?\n",
            "[Debug] Respuesta final: ¬°Qu√© bueno que est√©s contento! ¬øEn qu√© m√°s puedo ayudarte hoy?\n",
            "[Recibido] Usuario dijo: suficiente jeje\n",
            "[Debug] Respuesta OpenAI (GPT 3.5 turbo): ¬°Me alegra haberte sido de ayuda! Si tienes alguna otra pregunta, ¬°estar√© aqu√≠ para ayudarte! üòä\n",
            "[Debug] Respuesta OpenAI (GPT 4 turbo): ¬°Entendido! ¬øEn qu√© m√°s puedo ayudarte hoy?\n",
            "[Debug] Respuesta final: ¬°Me alegra haber sido de ayuda! Si necesitas algo m√°s, estoy aqu√≠ para asistirte. üòä ¬øEn qu√© m√°s puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "# Configurar el bot de Telegram y los handlers\n",
        "updater = Updater(telegram_token, use_context=True)\n",
        "dispatcher = updater.dispatcher\n",
        "\n",
        "# Registrar el manejador para mensajes de texto (no comandos)\n",
        "dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_message))\n",
        "\n",
        "# Iniciar el polling del bot\n",
        "print(\"Iniciando el bot de Telegram... Esperando mensajes.\")\n",
        "updater.start_polling()\n",
        "updater.idle()  # Mantener el bot corriendo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aprendizajes y l√≠neas futuras**"
      ],
      "metadata": {
        "id": "VvrQjAonc0Rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Uso de API de Claude**\n",
        "\n",
        "Aunque inicialmente intentamos utilizar la API de pago de Claude (Anthropic) para enriquecer la diversidad de respuestas generadas por modelos de diferentes proveedores, la implementaci√≥n result√≥ inviable. A pesar de varios intentos, la integraci√≥n fallaba sistem√°ticamente debido a un error persistente que no pudimos resolver dentro del tiempo y recursos disponibles.\n",
        "\n",
        "Como consecuencia, optamos por adaptar la arquitectura del sistema para combinar respuestas de modelos distintos dentro del ecosistema de OpenAI, concretamente GPT-3.5 y GPT-4. Esta alternativa permiti√≥ mantener la funcionalidad principal del sistema ‚Äîla generaci√≥n de una respuesta consensuada‚Äî sin comprometer la estabilidad del servicio.\n",
        "\n",
        "A futuro, ser√≠a interesante explorar la combinaci√≥n de modelos de distintas compa√±√≠as, como Mistral o DeepSeek, lo que no solo aportar√≠a mayor robustez y variedad en los estilos de respuesta, sino que tambi√©n permitir√≠a evaluar el comportamiento de m√∫ltiples LLMs en contextos comparables."
      ],
      "metadata": {
        "id": "0gDBalmDc4ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Seguridad de las credenciales API**\n",
        "\n",
        "Un problema cr√≠tico identificado en el desarrollo actual del proyecto es la exposici√≥n directa de la clave de la API de OpenAI dentro del notebook. En su estado actual, esta clave se encuentra codificada de forma expl√≠cita en una de las celdas del *notebook*, lo que implica que cualquier persona con acceso al archivo puede visualizarla y utilizarla sin restricciones.\n",
        "\n",
        "Este problema tiene implicaciones tanto funcionales como econ√≥micas:\n",
        "\n",
        "1. **Riesgo financiero**: La clave de API de OpenAI est√° siendo financiada actualmente de forma personal por nosotros (Yago). La exposici√≥n de dicha clave permitir√≠a que terceros generen peticiones a la API con cargo a esa cuenta, lo cual podr√≠a generar costes inesperados y no autorizados.\n",
        "2. **Falta de control de acceso**: Cualquier persona que reciba el notebook (por ejemplo, por correo, Drive o GitHub) puede copiar o ejecutar la clave sin ning√∫n mecanismo de autenticaci√≥n.\n",
        "3. **Riesgos de seguridad y reputaci√≥n**: Si la clave se utiliza para realizar acciones automatizadas fuera del control del propietario, tambi√©n podr√≠a comprometer el uso leg√≠timo del servicio o incluso suponer una violaci√≥n de los t√©rminos de uso de OpenAI."
      ],
      "metadata": {
        "id": "bRSw0D4Kd1ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Especificidad del ChatBot**\n",
        "\n",
        "Los *chatbots* han dejado de ser una idea futurista para convertirse en una realidad ampliamente implementada. En la actualidad, existen numerosos asistentes conversacionales de prop√≥sito general, cada uno con diferentes niveles de utilidad y especializaci√≥n. Esta abundancia de opciones plantea el reto de c√≥mo diferenciar una nueva propuesta dentro de un ecosistema ya saturado.\n",
        "\n",
        "Una estrategia eficaz para aportar valor a√±adido ser√≠a realizar un proceso de *fine-tuning* del modelo base utilizando datos espec√≠ficos y relevantes para un contexto concreto. Por ejemplo, podr√≠a entrenarse el modelo con el contenido acad√©mico de las distintas asignaturas de una carrera universitaria. De esta forma, se desarrollar√≠a un asistente virtual educativo, capaz de responder dudas frecuentes de los estudiantes de manera r√°pida, precisa y personalizada.\n",
        "\n",
        "Este enfoque permitir√≠a al *chatbot* actuar como un complemento al profesor, especialmente √∫til para resolver consultas sencillas o recurrentes, que a menudo no justifican un contacto directo. As√≠, se aliviar√≠a la carga de los docentes y se mejorar√≠a la experiencia del alumno, promoviendo un aprendizaje m√°s aut√≥nomo y accesible."
      ],
      "metadata": {
        "id": "qTbJjUfYfKBG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}